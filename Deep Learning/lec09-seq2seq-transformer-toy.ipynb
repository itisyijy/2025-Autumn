{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# Imports and Setup"],"metadata":{"id":"ITy3t1ovVvOo"}},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IvVGA6grTfl5","executionInfo":{"status":"ok","timestamp":1764654263771,"user_tz":-540,"elapsed":6,"user":{"displayName":"Jeongyun Lee","userId":"06040472116594639035"}},"outputId":"428f7e93-65cf-4b7b-b998-deea772f3bd7"},"outputs":[{"output_type":"stream","name":"stdout","text":["Using device: cuda\n"]}],"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import math\n","import random\n","\n","# Set random seed for reproducibility\n","SEED = 1234\n","random.seed(SEED)\n","torch.manual_seed(SEED)\n","torch.cuda.manual_seed(SEED)\n","torch.backends.cudnn.deterministic = True\n","\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","print(f\"Using device: {device}\")"]},{"cell_type":"markdown","source":["# Positional Encoding Module"],"metadata":{"id":"kj6LYv0PV2sQ"}},{"cell_type":"code","source":["class PositionalEncoding(nn.Module):\n","    \"\"\"\n","    Injects some information about the relative or absolute position of the tokens\n","    in the sequence. The positional encodings have the same dimension as the embeddings.\n","    \"\"\"\n","    def __init__(self, d_model, dropout=0.1, max_len=5000):\n","        super().__init__()\n","        self.dropout = nn.Dropout(p=dropout)\n","\n","        # Create constant 'pe' matrix with values dependent on pos and i\n","        pe = torch.zeros(max_len, d_model)\n","        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n","        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n","\n","        pe[:, 0::2] = torch.sin(position * div_term)\n","        pe[:, 1::2] = torch.cos(position * div_term)\n","\n","        # Register as buffer (not a learnable parameter, but part of state_dict)\n","        self.register_buffer('pe', pe.unsqueeze(0))\n","\n","    def forward(self, x):\n","        # x: [Batch, Seq_len, Dim]\n","        # Add positional encoding to the input embedding\n","        x = x + self.pe[:, :x.size(1), :]\n","        return self.dropout(x)"],"metadata":{"id":"ifLHDJ1wV9XI","executionInfo":{"status":"ok","timestamp":1764654315495,"user_tz":-540,"elapsed":3,"user":{"displayName":"Jeongyun Lee","userId":"06040472116594639035"}}},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":["# Seq2Seq Transformer Model Definition"],"metadata":{"id":"iUme0x6XWB6J"}},{"cell_type":"code","source":["class Seq2SeqTransformer(nn.Module):\n","    def __init__(self, num_tokens, dim_model, num_heads, num_encoder_layers, num_decoder_layers, dropout_p=0.1):\n","        super().__init__()\n","\n","        # Embedding layer and Positional Encoding\n","        self.embedding = nn.Embedding(num_tokens, dim_model)\n","        self.pos_encoder = PositionalEncoding(dim_model, dropout_p)\n","\n","        # Core: nn.Transformer module (contains both Encoder and Decoder)\n","        # batch_first=True ensures input format is [Batch, Seq, Dim]\n","        self.transformer = nn.Transformer(\n","            d_model=dim_model,\n","            nhead=num_heads,\n","            num_encoder_layers=num_encoder_layers,\n","            num_decoder_layers=num_decoder_layers,\n","            dropout=dropout_p,\n","            batch_first=True\n","        )\n","\n","        # Final output projection layer\n","        self.out = nn.Linear(dim_model, num_tokens)\n","        self.dim_model = dim_model\n","\n","    def forward(self, src, tgt, src_mask=None, tgt_mask=None, src_padding_mask=None, tgt_padding_mask=None):\n","        \"\"\"\n","        src: Source sequence\n","        tgt: Target sequence (shifted right)\n","        src_mask: Mask for source (usually None or all zeros)\n","        tgt_mask: Mask for target (causal mask to hide future tokens)\n","        src/tgt_padding_mask: Bool mask where True indicates padding tokens\n","        \"\"\"\n","        # 1. Apply embedding and positional encoding\n","        src = self.pos_encoder(self.embedding(src) * math.sqrt(self.dim_model))\n","        tgt = self.pos_encoder(self.embedding(tgt) * math.sqrt(self.dim_model))\n","\n","        # 2. Pass through the Transformer\n","        # Note: In PyTorch's nn.Transformer, padding masks are named *_key_padding_mask\n","        output = self.transformer(\n","            src, tgt,\n","            src_mask=src_mask,\n","            tgt_mask=tgt_mask,\n","            src_key_padding_mask=src_padding_mask,\n","            tgt_key_padding_mask=tgt_padding_mask,\n","            memory_key_padding_mask=src_padding_mask # Mask encoder padding for the decoder\n","        )\n","\n","        # 3. Project to vocabulary size\n","        return self.out(output)"],"metadata":{"id":"fuJdyLQhV1oB","executionInfo":{"status":"ok","timestamp":1764654494766,"user_tz":-540,"elapsed":3,"user":{"displayName":"Jeongyun Lee","userId":"06040472116594639035"}}},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":["# Masking Utilities"],"metadata":{"id":"dOHbsGirWJC6"}},{"cell_type":"code","source":["def generate_square_subsequent_mask(sz, device):\n","    \"\"\"\n","    Generates a causal mask (look-ahead mask) for the decoder.\n","    It prevents positions from attending to subsequent positions.\n","    \"\"\"\n","    mask = (torch.triu(torch.ones((sz, sz), device=device)) == 1).transpose(0, 1)\n","    mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n","    return mask\n","\n","def create_mask(src, tgt, device):\n","    \"\"\"\n","    Creates both the causal mask for the target and padding masks for both source and target.\n","    \"\"\"\n","    src_seq_len = src.shape[1]\n","    tgt_seq_len = tgt.shape[1]\n","\n","    # Target requires a causal mask\n","    tgt_mask = generate_square_subsequent_mask(tgt_seq_len, device)\n","\n","    # Source does not require a causal mask (encoder sees all tokens)\n","    src_mask = torch.zeros((src_seq_len, src_seq_len), device=device).type(torch.bool)\n","\n","    # Padding masks (identify where the token is 0)\n","    src_padding_mask = (src == 0)\n","    tgt_padding_mask = (tgt == 0)\n","\n","    return src_mask, tgt_mask, src_padding_mask, tgt_padding_mask"],"metadata":{"id":"5RndneWpWM-7","executionInfo":{"status":"ok","timestamp":1764654411276,"user_tz":-540,"elapsed":41,"user":{"displayName":"Jeongyun Lee","userId":"06040472116594639035"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["mask = generate_square_subsequent_mask(5, 'cpu')\n","print(mask)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qdrL7S8npApN","executionInfo":{"status":"ok","timestamp":1764655131923,"user_tz":-540,"elapsed":30,"user":{"displayName":"Jeongyun Lee","userId":"06040472116594639035"}},"outputId":"dd93a5c4-48b8-4c6d-fb7a-bfde3b891d17"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[0., -inf, -inf, -inf, -inf],\n","        [0., 0., -inf, -inf, -inf],\n","        [0., 0., 0., -inf, -inf],\n","        [0., 0., 0., 0., -inf],\n","        [0., 0., 0., 0., 0.]])\n"]}]},{"cell_type":"markdown","source":["cannot go to future(-inf)."],"metadata":{"id":"lNgnQnEkpPyN"}},{"cell_type":"markdown","source":["# Hyperparameters and Model Initialization"],"metadata":{"id":"ngOfbe0xWPQu"}},{"cell_type":"code","source":["\n","# Special Tokens\n","PAD_IDX = 0\n","SOS_IDX = 1\n","EOS_IDX = 2\n","\n","# Hyperparameters\n","VOCAB_SIZE = 20    # 0~19\n","DIM_MODEL = 128\n","NUM_HEADS = 4\n","NUM_LAYERS = 2\n","BATCH_SIZE = 64\n","MAX_LEN = 12\n","\n","# Initialize Model (Dropout=0.0 for toy task specific optimization)\n","model = Seq2SeqTransformer(\n","    VOCAB_SIZE, DIM_MODEL, NUM_HEADS, NUM_LAYERS, NUM_LAYERS, dropout_p=0.0\n",").to(device)\n","\n","criterion = nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n","optimizer = optim.Adam(model.parameters(), lr=0.0005)\n","\n","print(f\"Model Initialized. Parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QYjreyQZWSFZ","executionInfo":{"status":"ok","timestamp":1764655037644,"user_tz":-540,"elapsed":5181,"user":{"displayName":"Jeongyun Lee","userId":"06040472116594639035"}},"outputId":"0a04f7f8-b981-4b87-b055-4c1f39964a1a"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["Model Initialized. Parameters: 2,510,356\n"]}]},{"cell_type":"markdown","source":["# Data Generation (Toy Task: Reverse Sequence)"],"metadata":{"id":"JwoJ3ssiWUVZ"}},{"cell_type":"code","source":["def get_batch(bsz):\n","    \"\"\"\n","    Generates a batch of random sequences.\n","    Task: Reverse the input sequence.\n","    Input:  [1, 5, 2, 0] (0 is padding)\n","    Target: [SOS, 2, 5, 1, EOS]\n","    \"\"\"\n","    data = []\n","    targets = []\n","\n","    for _ in range(bsz):\n","        # Length between 3 and (MAX_LEN - 2) to fit SOS and EOS\n","        seq_len = random.randint(3, MAX_LEN - 2)\n","\n","        # Random sequence (3 ~ 19 range to avoid collision with special tokens)\n","        seq = [random.randint(3, VOCAB_SIZE-1) for _ in range(seq_len)]\n","\n","        # Source: Sequence + Padding\n","        src = seq + [PAD_IDX] * (MAX_LEN - len(seq))\n","\n","        # Target Sequence (Reverse)\n","        tgt_seq = seq[::-1]\n","\n","        # Target Input: SOS + Reversed Seq + Padding\n","        tgt_input = [SOS_IDX] + tgt_seq + [PAD_IDX] * (MAX_LEN - len(seq) - 1)\n","\n","        # Target Output: Reversed Seq + EOS + Padding\n","        tgt_out = tgt_seq + [EOS_IDX] + [PAD_IDX] * (MAX_LEN - len(seq) - 1)\n","\n","        data.append(src)\n","        targets.append((tgt_input, tgt_out))\n","\n","    src = torch.tensor(data).to(device)\n","    tgt_input = torch.tensor([t[0] for t in targets]).to(device)\n","    tgt_out = torch.tensor([t[1] for t in targets]).to(device)\n","\n","    return src, tgt_input, tgt_out"],"metadata":{"id":"B047EHP7WXwJ","executionInfo":{"status":"ok","timestamp":1764655333751,"user_tz":-540,"elapsed":3,"user":{"displayName":"Jeongyun Lee","userId":"06040472116594639035"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["src, tgt_in, tgt_out = get_batch(4)\n","print(src)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UFadpxsUpxoF","executionInfo":{"status":"ok","timestamp":1764655377526,"user_tz":-540,"elapsed":7,"user":{"displayName":"Jeongyun Lee","userId":"06040472116594639035"}},"outputId":"52a1cb4f-0070-43ae-c079-7cf87af68c02"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[14, 11, 10,  9, 17, 19,  3, 19,  5,  3,  0,  0],\n","        [ 7, 18, 15,  8,  4,  5, 18,  8,  0,  0,  0,  0],\n","        [ 9,  7,  3, 14,  0,  0,  0,  0,  0,  0,  0,  0],\n","        [19,  7, 12, 17,  0,  0,  0,  0,  0,  0,  0,  0]], device='cuda:0')\n"]}]},{"cell_type":"code","source":["# SOS + rev + PAD\n","print(tgt_in)\n","# SOS, 2, 3, 1 => # 2, 3, 1, EOS"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"79pmMQSip_7V","executionInfo":{"status":"ok","timestamp":1764655385545,"user_tz":-540,"elapsed":28,"user":{"displayName":"Jeongyun Lee","userId":"06040472116594639035"}},"outputId":"9fca3457-7a89-4426-a7f8-17a9a5eb3b01"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[ 1,  3,  5, 19,  3, 19, 17,  9, 10, 11, 14,  0],\n","        [ 1,  8, 18,  5,  4,  8, 15, 18,  7,  0,  0,  0],\n","        [ 1, 14,  3,  7,  9,  0,  0,  0,  0,  0,  0,  0],\n","        [ 1, 17, 12,  7, 19,  0,  0,  0,  0,  0,  0,  0]], device='cuda:0')\n"]}]},{"cell_type":"markdown","source":[],"metadata":{"id":"HReutv5EqsBu"}},{"cell_type":"code","source":["# GT of decoder: rev + EOS + PAD\n","# for calculate loss\n","print(tgt_out)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dq4zilW7qFLF","executionInfo":{"status":"ok","timestamp":1764655856073,"user_tz":-540,"elapsed":10,"user":{"displayName":"Jeongyun Lee","userId":"06040472116594639035"}},"outputId":"d6c9b907-bb2f-4594-9b52-97da7dd80a52"},"execution_count":18,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[ 3,  5, 19,  3, 19, 17,  9, 10, 11, 14,  2,  0],\n","        [ 8, 18,  5,  4,  8, 15, 18,  7,  2,  0,  0,  0],\n","        [14,  3,  7,  9,  2,  0,  0,  0,  0,  0,  0,  0],\n","        [17, 12,  7, 19,  2,  0,  0,  0,  0,  0,  0,  0]], device='cuda:0')\n"]}]},{"cell_type":"markdown","source":["# Training Loop"],"metadata":{"id":"DikaKbJPWbYV"}},{"cell_type":"code","source":["model.train()\n","print(\"Training Start...\")\n","\n","EPOCHS = 3000\n","\n","for epoch in range(EPOCHS):\n","    # 1. Get batch data\n","    src, tgt_input, tgt_out = get_batch(BATCH_SIZE)\n","\n","    # 2. Create masks\n","    src_mask, tgt_mask, src_pad_mask, tgt_pad_mask = create_mask(src, tgt_input, device)\n","\n","    optimizer.zero_grad()\n","\n","    # 3. Forward pass\n","    logits = model(src, tgt_input, src_mask, tgt_mask, src_pad_mask, tgt_pad_mask)\n","\n","    # 4. Calculate Loss\n","    # Flatten output to [Batch * Seq, Vocab] for CrossEntropyLoss\n","    loss = criterion(logits.reshape(-1, VOCAB_SIZE), tgt_out.reshape(-1))\n","\n","    # 5. Backward pass and Optimization\n","    loss.backward()\n","    optimizer.step()\n","\n","    if epoch % 50 == 0:\n","        print(f\"Epoch {epoch:3d} | Loss: {loss.item():.4f}\")\n","\n","print(\"Training Complete.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iS6ynwYSWdKt","executionInfo":{"status":"ok","timestamp":1764655912076,"user_tz":-540,"elapsed":46384,"user":{"displayName":"Jeongyun Lee","userId":"06040472116594639035"}},"outputId":"ebb15905-3707-4093-ac35-a74acb1127f7"},"execution_count":19,"outputs":[{"output_type":"stream","name":"stdout","text":["Training Start...\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/torch/nn/functional.py:6044: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["Epoch   0 | Loss: 3.1807\n","Epoch  50 | Loss: 1.6873\n","Epoch 100 | Loss: 1.5163\n","Epoch 150 | Loss: 1.4808\n","Epoch 200 | Loss: 1.3423\n","Epoch 250 | Loss: 1.2897\n","Epoch 300 | Loss: 1.1034\n","Epoch 350 | Loss: 0.9643\n","Epoch 400 | Loss: 0.8307\n","Epoch 450 | Loss: 0.6528\n","Epoch 500 | Loss: 0.6369\n","Epoch 550 | Loss: 0.5581\n","Epoch 600 | Loss: 0.4480\n","Epoch 650 | Loss: 0.3925\n","Epoch 700 | Loss: 0.3075\n","Epoch 750 | Loss: 0.2828\n","Epoch 800 | Loss: 0.3065\n","Epoch 850 | Loss: 0.1718\n","Epoch 900 | Loss: 0.2212\n","Epoch 950 | Loss: 0.1399\n","Epoch 1000 | Loss: 0.1058\n","Epoch 1050 | Loss: 0.1404\n","Epoch 1100 | Loss: 0.4015\n","Epoch 1150 | Loss: 0.1511\n","Epoch 1200 | Loss: 0.0985\n","Epoch 1250 | Loss: 0.1048\n","Epoch 1300 | Loss: 0.0421\n","Epoch 1350 | Loss: 0.2300\n","Epoch 1400 | Loss: 0.0781\n","Epoch 1450 | Loss: 0.0633\n","Epoch 1500 | Loss: 0.0465\n","Epoch 1550 | Loss: 0.0316\n","Epoch 1600 | Loss: 0.1687\n","Epoch 1650 | Loss: 0.0456\n","Epoch 1700 | Loss: 0.2620\n","Epoch 1750 | Loss: 0.0920\n","Epoch 1800 | Loss: 0.0243\n","Epoch 1850 | Loss: 0.0538\n","Epoch 1900 | Loss: 0.0381\n","Epoch 1950 | Loss: 0.0410\n","Epoch 2000 | Loss: 0.0176\n","Epoch 2050 | Loss: 0.0370\n","Epoch 2100 | Loss: 0.2332\n","Epoch 2150 | Loss: 0.0450\n","Epoch 2200 | Loss: 0.0670\n","Epoch 2250 | Loss: 0.0543\n","Epoch 2300 | Loss: 0.0942\n","Epoch 2350 | Loss: 0.0507\n","Epoch 2400 | Loss: 0.0271\n","Epoch 2450 | Loss: 0.0329\n","Epoch 2500 | Loss: 0.0131\n","Epoch 2550 | Loss: 0.0138\n","Epoch 2600 | Loss: 0.0383\n","Epoch 2650 | Loss: 0.3548\n","Epoch 2700 | Loss: 0.0247\n","Epoch 2750 | Loss: 0.0401\n","Epoch 2800 | Loss: 0.0155\n","Epoch 2850 | Loss: 0.0241\n","Epoch 2900 | Loss: 0.1092\n","Epoch 2950 | Loss: 0.2633\n","Training Complete.\n"]}]},{"cell_type":"markdown","source":["# Inference (Greedy Decoding)"],"metadata":{"id":"ebIKtmKnWlwO"}},{"cell_type":"code","source":["\n","def greedy_decode(model, src, max_len, start_symbol):\n","    src = src.to(device)\n","    # Encoder Masking\n","    src_padding_mask = (src == PAD_IDX).to(device)\n","\n","    memory = model.transformer.encoder(\n","        model.pos_encoder(model.embedding(src) * math.sqrt(DIM_MODEL)),\n","        src_key_padding_mask=src_padding_mask\n","    )\n","\n","    ys = torch.ones(1, 1).fill_(start_symbol).type(torch.long).to(device)\n","\n","    for i in range(max_len-1):\n","        tgt_mask = generate_square_subsequent_mask(ys.size(1), device)\n","\n","        out = model.transformer.decoder(\n","            model.pos_encoder(model.embedding(ys) * math.sqrt(DIM_MODEL)),\n","            memory,\n","            tgt_mask=tgt_mask,\n","            memory_key_padding_mask=src_padding_mask\n","        )\n","\n","        prob = model.out(out[:, -1])\n","        _, next_word = torch.max(prob, dim=1)\n","        next_word = next_word.item()\n","\n","        ys = torch.cat([ys, torch.ones(1, 1).type_as(src.data).fill_(next_word)], dim=1)\n","\n","        if next_word == EOS_IDX:\n","            break\n","\n","    return ys\n","\n","# --- Run Inference ---\n","model.eval()\n","print(\"\\nTesting (Source -> Reversed Prediction):\")\n","\n","# Test Sequence\n","test_src_list = [3, 4, 5, 6, 7, 8, 9]\n","print(f\"Input Sequence: {test_src_list}\")\n","\n","src = torch.tensor([test_src_list + [PAD_IDX]*(MAX_LEN - len(test_src_list))]).to(device)\n","\n","# Perform inference\n","pred_tensor = greedy_decode(model, src, MAX_LEN, start_symbol=SOS_IDX)\n","\n","# Post-processing\n","result = pred_tensor.squeeze().tolist()\n","print(f\"Raw Output: {result}\")\n","\n","# Clean up output\n","final_res = []\n","for token in result:\n","    if token == SOS_IDX: continue\n","    if token == EOS_IDX: break\n","    final_res.append(token)\n","\n","print(f\"Predicted Result: {final_res}\")\n","\n","# Check correctness\n","if final_res == test_src_list[::-1]:\n","    print(\"✅ Success! The sequence is correctly reversed.\")\n","else:\n","    print(\"❌ Failed.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tQGg8cj7Wg2Q","executionInfo":{"status":"ok","timestamp":1764656012572,"user_tz":-540,"elapsed":219,"user":{"displayName":"Jeongyun Lee","userId":"06040472116594639035"}},"outputId":"44a207a4-2c20-436d-f51f-5792b4c8a598"},"execution_count":20,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Testing (Source -> Reversed Prediction):\n","Input Sequence: [3, 4, 5, 6, 7, 8, 9]\n","Raw Output: [1, 9, 8, 7, 6, 5, 4, 3, 2]\n","Predicted Result: [9, 8, 7, 6, 5, 4, 3]\n","✅ Success! The sequence is correctly reversed.\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"6BSpQFIxWZbo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"ry7raEa3WTZj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"3gcy2s4fWIaU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"ukyw9YqCTkY9"},"execution_count":null,"outputs":[]}]}